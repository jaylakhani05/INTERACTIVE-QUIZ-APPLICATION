# INTERACTIVE-QUIZ-APPLICATION

Company: CODTECH IT SOLUTION

NAME: JAY LAKHANI

INTERN ID: CT06DF2743

DOMAIN: DATA ANAYTICS

DURATION: 6 WEEK

MENTOR: NEELA SANTOSH

Throughout the CODTECH Data Analysis Internship, I had the opportunity to work on four well-structured and practical tasks that encompassed the fundamental domains of modern data analysis. These tasks included Exploratory Data Analysis (EDA), Data Cleaning and Preprocessing, Data Visualization, and Sentiment Analysis using Natural Language Processing (NLP). Each task allowed me to gain hands-on experience with real-world datasets and apply essential Python libraries and data science tools to derive meaningful insights and solutions.

In Task 1 focused on Exploratory Data Analysis (EDA), where the main goal was to understand the dataset's structure and identify patterns, trends, and potential anomalies. I began by importing and exploring the dataset using the pandas library to examine column names, data types, and statistical summaries using .info() and .describe() functions. I also used matplotlib and seaborn to visualize the data through bar plots, histograms, and correlation heatmaps. These visual tools helped reveal the relationships between variables, frequency distributions, and potential outliers. The EDA process set the foundation for all subsequent tasks by enabling me to understand what kind of data I was working with and how it could be used to extract insights.

In Task 2, I worked on Data Cleaning and Preprocessing, which is one of the most crucial steps in data analysis. Raw data often contains noise, missing values, and inconsistent formatting. I cleaned the data by identifying and handling null values using fillna() or dropna(), removed duplicates using .drop_duplicates(), and converted data types when needed. Additionally, I performed categorical data encoding through Label Encoding and One-Hot Encoding using tools from sklearn.preprocessing, which made non-numeric data usable in machine learning algorithms. This task reinforced the importance of data integrity and ensured that the dataset was ready for modeling and visualization.

In Task 3, Data Visualization, required me to create meaningful visual representations of data to convey insights more effectively. Using matplotlib and seaborn, I built various charts such as bar graphs, pie charts, scatter plots, and line graphs. These visualizations helped in identifying trends, comparisons, and outliers. For instance, I used pie charts to show proportional data, bar charts to compare categorical variables, and correlation heatmaps to highlight strong linear relationships between numeric variables. This task emphasized the power of visual storytelling and demonstrated how complex data can be made accessible and engaging through simple visuals.

The final task, Task 4, centered around Sentiment Analysis using Natural Language Processing (NLP). I worked with an IMDb movie review dataset containing textual reviews and their corresponding sentiment labels: positive, negative, or neutral. First, I used the nltk library to clean the reviews by removing HTML tags, stopwords, punctuation, and non-alphabetic characters. The cleaned text was then vectorized using TfidfVectorizer from sklearn.feature_extraction.text, converting it into numerical format suitable for model training. I trained a LogisticRegression model from sklearn.linear_model and evaluated it using classification_report to determine accuracy, precision, and recall. I also created a function to classify custom reviews using the trained model. Additionally, I calculated the total number of positive, negative, and neutral reviews from the data without using machine learning, which provided a quick overview of sentiment distribution. This task showcased how unstructured text data can be converted into actionable insights using NLP techniques.


